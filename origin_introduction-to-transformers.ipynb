{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png\" width=\"500\" height=\"600\"></center>\n\n# <center><b><span style='color:#FF9B9B'>Introduction to Transformers</span></b></center>\n\n### <b><span style='color:#FF9B9B'>Credits</span></b>\n\nThis notebook is based on the [Pytorch Transformers from Scratch](https://www.youtube.com/watch?v=U0s0f995w14) video by Aladdin Persson and the [How to get meaning from text with language model BERT ](https://www.youtube.com/watch?v=-9vVhYEXeyQ) video by Peltarion. \n\nGo give them the thumbs up üëç!\n\n### <b><span style='color:#FF9B9B'>What will you learn?</span></b>\n\n- The theoretical background on how Transformers work.\n- Learn the different components of the Transformer architecture, like the attention mechanism.\n- Learn how to code all the different building blocks, like the Encoder block and the Decoder block.\n\n\n### <b><span style='color:#FF9B9B'>Table of Contents</span></b><a class='anchor' id='top'>\n\nThe notebook will be divided into separate sections to provide an organized walk through for the process used. This process can be modified for individual use cases. The sections are:\n\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">  \n<ol>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#explained\">Transformers Explained Step by Step</a></li>\n    <ul>\n        <li><a href=\"#tokenization\">Tokenization</a></li>\n        <li><a href=\"#embedding\">Embedding</a></li>\n        <li><a href=\"#context\">Context</a></li>\n        <li><a href=\"#attention\">Self-Attention Mechanism</a></li>\n        <li><a href=\"#kqv\">Keys, Queries and Values</a></li>\n        <li><a href=\"#multi_head\">Multi-head Attention</a></li>\n        <li><a href=\"#positional_encoding\">Positional Encoding</a></li>\n        <li><a href=\"#bert\">BERT</a></li>\n    </ul>\n<li><a href=\"#multi_head_attention\">Multi-Head Attention</a></li>\n<li><a href=\"#encoder_layer\">Encoder Layer</a></li>\n<li><a href=\"#encoder\">Encoder</a></li>\n<li><a href=\"#decoder_block\">Decoder Layer</a></li>\n<li><a href=\"#decoder\">Decoder</a></li>\n<li><a href=\"#transformer\">Transformer</a></li>\n<li><a href=\"#example\">Example</a></li>\n</ol> \n</div> ","metadata":{}},{"cell_type":"markdown","source":"# <b>1 <span style='color:#FF9B9B'>|</span> Introduction</b><a class='anchor' id='introduction'></a> [‚Üë](#top) \n***\n\nIn this notebook we will:\n- First, get the theoretical background of how Transformers work and which are their basic components.\n- Second, code all the different blocks of the Transformer architecture to finally build the Transformer.\n\nIn this tutorial we will build a Transformer from scratch using PyTorch. To get a better understanding of Transformers these are some good readings and videos:\n1. [Transformers from Scratch by Peter Bloem](https://peterbloem.nl/blog/transformers)\n2. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n3. [How to get meaning from text with language model BERT](https://www.youtube.com/watch?v=-9vVhYEXeyQ)\n4. [PyTorch Transformers from Scratch](https://www.youtube.com/watch?v=U0s0f995w14)\n5. [GitHub with Transformer code](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py)\n6. [A detailed guide to PyTorch‚Äôs nn.Transformer() module](https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1)","metadata":{}},{"cell_type":"code","source":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('-9vVhYEXeyQ', width=800, height=300)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:37.931823Z","iopub.execute_input":"2023-07-19T20:08:37.932307Z","iopub.status.idle":"2023-07-19T20:08:38.012576Z","shell.execute_reply.started":"2023-07-19T20:08:37.932247Z","shell.execute_reply":"2023-07-19T20:08:38.011161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('U0s0f995w14', width=800, height=300)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:38.014672Z","iopub.execute_input":"2023-07-19T20:08:38.015093Z","iopub.status.idle":"2023-07-19T20:08:38.046967Z","shell.execute_reply.started":"2023-07-19T20:08:38.01506Z","shell.execute_reply":"2023-07-19T20:08:38.046099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#FF9B9B'>The Transformer Network</span></b>\n\nThis is the diagram of the Transformer network presented in the *Attention is All You Need* paper. We will go through all the different pieces of this network throughout this notebook.\n\n<center><img src=\"https://www.researchgate.net/publication/344197785/figure/fig2/AS:934416989843456@1599793779015/Transformer-model-architecture-described-in-Attention-Is-All-You-Need-6.ppm\" width=\"350\"></center>\n","metadata":{}},{"cell_type":"markdown","source":"# <b>2 <span style='color:#FF9B9B'>|</span> Transformers Explained Step by Step</b><a class='anchor' id='explained'></a> [‚Üë](#top) \n***\n\n### <b><span style='color:#FF9B9B'>Tokenization</span></b><a class='anchor' id='tokenization'></a> [‚Üë](#top)\n\nThe first step in processing text is to cut it into pieces called **tokens**.  There are many variations of how to do it, and we won‚Äôt go into details, but `BERT` uses `WordPiece` tokenization.  This means that tokens correspond roughly to words and punctuation, although a word can also be split into several  tokens if it contains a common prefix or suffix. These are called sub-word tokens and usually contain `##` characters. Words can even be spelled out if they have never been seen before.\n<br>\n<center><img src=\"https://i.imgur.com/kCoLZuG.png\" width=\"750\"></center>\n\n### <b><span style='color:#FF9B9B'>Embedding</span></b><a class='anchor' id='embedding'></a> [‚Üë](#top)\n\nThe second step is to associate each token  with an **embedding**, which is nothing more than a vector of real numbers. Again, there are many ways to create embedding vectors. Fortunately, already trained embeddings are often provided by research groups, and we can just use an existing dictionary to convert the WordPiece tokens into embedding vectors. \n<br>\n<center><img src=\"https://i.imgur.com/NulRCFU.png\" width=\"750\"></center>\n<br>\nThe embedding of tokens into vectors is an achievement in itself. The values inside an embedding carry information about the meaning of the token, but they are also arranged in such a way that one can perform mathematical operations on them, which correspond to semantic changes, like changing the gender of a noun, or the tense of a verb, or even the homeland of a city.<br>\n\n<center><img src=\"https://i.imgur.com/6LtQ1Pd.png\" width=\"750\"></center>\n\n### <b><span style='color:#FF9B9B'>Context</span></b><a class='anchor' id='context'></a> [‚Üë](#top)\n\nHowever, embeddings are associated with tokens by a straight dictionary look-up, which means that the same token always gets the same embedding, regardless of its context. This is where the attention mechanism comes in, and specifically for BERT, the scaled dot-product self-attention. Attention transforms the default embeddings by analyzing the whole sequence of tokens, so that the values are more representative of the token they represent in the context of the sentence.\n\n<center><img src=\"https://i.imgur.com/oPfudSt.png\" width=\"750\"></center>\n\n### <b><span style='color:#FF9B9B'>Self Attention Mechanism</span></b><a class='anchor' id='attention'></a> [‚Üë](#top)\n\nLet's have a look at this process with the sequence of tokens `walk`, `by`, `river`, `bank`. Each token is initially replaced by its default embedding, which in this case is a vector with 768 components. \n<br>\n<center><img src=\"https://i.imgur.com/HHZDysd.png\" width=\"450\"></center>\n<br>\n\nLet's color the embedding of the first token to follow what happens to it. We start by calculating the scalar product between pairs of embeddings. Here we have the first embedding with itself. When the two vectors are more correlated, or aligned, meaning that they are generally more similar, the scalar product is higher (darker in image), and we consider that they have a strong relationship. If they had less similar content, the scalar product would be lower (brighter in the image) and we would consider that they don't relate to each other.\n\n<br>\n<center><img src=\"https://i.imgur.com/z3s8TPe.png\" width=\"450\"></center>\n<br>\n\nWe go on and calculate the scalar product for every possible pair of embedding vectors in the input sequence. The values obtained are usually scaled down to avoid getting large values, which improves the numerical behavior. That‚Äôs done here by dividing by the square root of 768, which is the size of the vectors. \n<br>\n<center><img src=\"https://i.imgur.com/ngHnOUc.png\" width=\"450\"></center>\n<br>\n \nThen comes the only non-linear operation in the attention mechanism: The scaled values are passed through a softmax activation function, by groups corresponding to each input token. So in this illustration, we apply the softmax column by column. What the softmax does is to exponentially amplify large values, while crushing low and negative values towards zero. It also does normalization, so that each column sums up to 1. \n\n<br>\n<center><img src=\"https://i.imgur.com/pLl50D7.png\" width=\"450\"></center>\n<br>\n\nFinally, we create a new embedding vector for each token by linear combination of the input embeddings, in proportions given by the softmax results. We can say that the new embedding vectors are contextualized, since they contain a fraction of every input embedding for this particular sequence of tokens. In particular, if a token has a strong relationship with another one, a large fraction of its new contextualized embedding will be made of the related embedding. If a token doesn‚Äôt relate much to any other, as measured by the scalar product between their input embeddings, its contextualized embedding will be nearly identical to the input embedding.\n\n<br>\n<center><img src=\"https://i.imgur.com/UPZy2nm.png\" width=\"450\"></center>\n<br>\n\nFor instance, one can imagine that the vector space has a direction that corresponds to the idea of *nature*. The input embeddings of the  tokens `river` and `bank` should both have large values in that direction, so that they are more similar and have a strong relationship. As a result, the new contextualized embeddings of the `river` and `bank` tokens would combine both input embeddings in roughly equal parts. On the other hand, the preposition `by` sounds quite neutral, so that its embedding should have a weak relationship with every other one and little modification of its embedding vector would occur. So there we have the mechanism that lets the scaled dot-product attention utilize context. \n\n\nTo recap:\n1. First, it determines how much the  input embedding vectors relate to  each other using the scalar product.\n2. The results are then scaled down, and the softmax activation function is applied, which normalizes these results in a non-linear way.\n3. New contextualized embeddings are finally created for every token by linear combination of all the input embeddings, using the softmax proportions as coefficients \n<br>\n<center><img src=\"https://i.imgur.com/qfFLyND.gif\" width=\"950\"></center>\n<br>\n\n### <b><span style='color:#FF9B9B'>Keys, Queries and Values</span></b><a class='anchor' id='kqv'></a> [‚Üë](#top)\n\nHowever, that's not the whole story. Most importantly, we don't have to use the input embedding vectors as is. We can first project them using 3 linear projections to create the so-called *Key*, *Query*, and *Value* vectors. Typically, the projections are also mapping the input embeddings onto a space of lower dimension. In the case of BERT, the Key, Query, and Value vectors all have 64 components.\n<br>\n<center><img src=\"https://i.imgur.com/lIhueb8.png\" width=\"450\"></center>\n<br>\n\nEach projection can be thought of as focusing on different directions of the vector space, which would represent different semantic aspects. One can imagine that a Key is the projection of an embedding onto the direction of \"prepositions\", and a Query is the projection of an  embedding along the direction of \"locations\". In this case, the Key of the token `by` should have a strong relationship with every other Query,  since `by` should have strong components in the direction of \"prepositions\", and every other token should have strong components in the direction of \"locations\". The Values can come from yet another projection that is relevant, for example the direction of physical places. It‚Äôs these values that are combined to create the contextualized embeddings In practice, the meaning of each projection may not be so clear, and the model is free to learn whatever projections allow it to solve language tasks the most efficiently.\n\n### <b><span style='color:#FF9B9B'>Multi-head Attention</span></b><a class='anchor' id='multi_head'></a> [‚Üë](#top)\n\nIn addition, the same process can be repeated many times with different Key, Query, and Value projections, forming  what is called a **multi-head attention**. Each head can focus on different projections of the input embeddings. For instance, one head could  calculate the preposition/location relationships, while another head could calculate subject/verb relationships, simply by using different projections to create the Key, Query, and Value vectors. The outputs from each head are concatenated back in a large vector. BERT uses 12 such heads, which means that the final output contains one 768-component contextualized embedding vector per token, equally long with the input.\n<br>\n<center><img src=\"https://i.imgur.com/pH4NcnC.png\" width=\"450\"></center>\n<br>\n\n### <b><span style='color:#FF9B9B'>Positional Encoding</span></b><a class='anchor' id='positional_encoding'></a> [‚Üë](#top)\n\nWe can also kickstart the process by adding the input embeddings to **positional embeddings**. Positional embeddings are vectors that contain information about a position in the sequence, rather than about the meaning of a token. This adds information about the sequence even before attention is applied, and it allows attention to calculate relationships knowing the relative order of the tokens.\n\n<br>\n<center><img src=\"https://i.imgur.com/vbCEp1G.png\" width=\"450\"></center>\n<br>\n \nA detailed explanation of how it works can be found [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/), but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won‚Äôt really need to understand: \n\n$$\\begin{equation}\n  p_{i,j} = \\left\\{\n  \\begin{array}{@{}ll@{}}\n    \\sin \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=even \\\\\n    \\cos \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=odd \\\\\n  \\end{array}\\right.\n\\end{equation} \n$$\n\n \n### <b><span style='color:#FF9B9B'>BERT</span></b><a class='anchor' id='bert'></a> [‚Üë](#top)\n \nFinally, thanks to the non-linearity introduced by the softmax function, we can achieve even more complex transformations  of the embeddings by applying attention again and again, with a couple of  helpful steps between each application. A complete model like BERT uses 12 layers of  attention, each with its own set of projections So when you search for suggestions for a \"walk by the river bank\", the computer doesn‚Äôt only get a chance to recognize the keyword ‚Äúriver‚Äù, but even the numerical values given to ‚Äúbank‚Äù indicate that you‚Äôre interested in enjoying the waterside, and not in need of the nearest cash machine.\n<br>\n<center><img src=\"https://i.imgur.com/Tn0ddHY.png\" width=\"650\"></center>\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"# <b>2 <span style='color:#FF9B9B'>|</span> Importing Libraries</b><a class='anchor' id='import_libraries'></a> [‚Üë](#top) \n***","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-19T20:08:38.048229Z","iopub.execute_input":"2023-07-19T20:08:38.048984Z","iopub.status.idle":"2023-07-19T20:08:40.284932Z","shell.execute_reply.started":"2023-07-19T20:08:38.048947Z","shell.execute_reply":"2023-07-19T20:08:40.283826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:#FF9B9B'>|</span> Multi-Head Attention</b><a class='anchor' id='multi_head_attention'></a> [‚Üë](#top) \n***\nAttention is a mechanism that allows neural networks to assign a different amount of weight or **attention** to each element in a sequence. For text sequences, the elements are token embeddings, where each token is mapped to a vector of some fixed dimension. For example, in BERT each\ntoken is represented as a 768-dimensional vector. The ‚Äúself‚Äù part of self-attention refers to the fact that these weights are computed for\nall hidden states in the same set‚Äîfor example, all the hidden states of the encoder. By contrast, the attention mechanism associated with\nrecurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep.\n\nThe main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a\nweighted average of each embedding. Another way to formulate this is to say that given a sequence of token embeddings $x_{1}, x_{2}, ..., x_{n}$, self-attention produces a sequence of new embeddings $x^{'}_{1}, x^{'}_{2}, ..., x^{'}_{n}$ where each $x^{'}_{i}$ is a linear combination of all the $x_{j}$:\n\n$$x^{'}_{i} = \\sum^{n}_{j=1} w_{ji} . x_{j}$$","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(MultiHeadAttention, self).__init__()\n        \"\"\"\n        MultiHeadAttention mechanism. The input of the MultiHeadAttention mechanism is an embedding (or sequence of embeddings).\n        The embeddings are split into different parts and each part is fed into a head.\n        :param embed_size: the size of the embedding.\n        :param heads: the number of heads you wish to create.\n        \"\"\"\n        self.embed_size = embed_size # 512 in Transformer  \n        self.heads = heads # 8 in Transformer\n        self.head_dim = embed_size // heads # 64 in Transformer\n        assert (\n            self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"\n        # === Project Embeddings into three vectors: Query, Key and Value ===\n        # Note: some implementations do: nn.Linear(embed_size, head_dim). We won't do this. We will project it \n        # on a space of size embed_size and then split it into N heads of head_dim shape.\n        self.values = nn.Linear(embed_size, embed_size)\n        self.keys = nn.Linear(embed_size, embed_size)\n        self.queries = nn.Linear(embed_size, embed_size)\n        self.fc_out = nn.Linear(embed_size, embed_size)\n\n    def forward(self, values, keys, query, mask):\n        # Values, Keys and Queries have size: (batch_size, sequence_len, embedding_size)\n        batch_size = query.shape[0]# Get number of training examples/batch size.\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n        # === Pass through Linear Layer ===\n        values = self.values(values)  # (batch_size, value_len, embed_size)\n        keys = self.keys(keys)  # (batch_size, key_len, embed_size)\n        queries = self.queries(query)  # (batch_size, query_len, embed_size)\n\n        # Split the embedding into self.heads different pieces\n        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n        queries = queries.reshape(batch_size, query_len, self.heads, self.head_dim)\n\n        # Einsum does matrix mult. for query*keys for each training example\n        # with every other training example, don't be confused by einsum\n        # it's just how I like doing matrix multiplication & bmm\n\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n        # queries shape: (batch_size, query_len, heads, heads_dim),\n        # keys shape: (batch_size, key_len, heads, heads_dim)\n        # energy: (batch_size, heads, query_len, key_len)\n\n        # Mask padded indices so their weights become 0\n        if mask is not None:\n            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n\n        # Normalize energy values similarly to seq2seq + attention\n        # so that they sum to 1. Also divide by scaling factor for\n        # better stability\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3) \n        # attention shape: (batch_size, heads, query_len, key_len)\n        # values shape: (batch_size, value_len, heads, heads_dim)\n        # out after matrix multiply: (batch_size, query_len, heads, head_dim), then\n        # we reshape and flatten the last two dimensions.\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n            batch_size, query_len, self.heads * self.head_dim\n        )\n        # Linear layer doesn't modify the shape, final shape will be\n        # (batch_size, query_len, embed_size)\n        out = self.fc_out(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.286744Z","iopub.execute_input":"2023-07-19T20:08:40.287432Z","iopub.status.idle":"2023-07-19T20:08:40.303492Z","shell.execute_reply.started":"2023-07-19T20:08:40.287382Z","shell.execute_reply":"2023-07-19T20:08:40.302316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:#FF9B9B'>|</span> Encoder Layer</b><a class='anchor' id='encoder_layer'></a> [‚Üë](#top) \n***\n\n<br>\n<center><img style=\"float:left; margin:20px; padding:20px; max-height:250px\" src=\"https://i.imgur.com/vUOhpoC.png\"></center>\n<br>\n\nWe will be referring to the encoder layer. The encoder layer/block consists of:\n1. *Multi-Head Attention*\n2. *Add & Norm*\n3. *Feed Forward*\n4. *Add & Norm* again.\n\n- [nn.LayerNorm()](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n- The `forward_expansion` is a parameter in the \"Attention is All You Need\" paper which simply adds nodes to the Linear Layer. Since it's used in two different layers in the end it doesn't affect the shape of the output (same as input) it just add some extra computation. Its default value is 4.","metadata":{}},{"cell_type":"code","source":"class TransformerLayer(nn.Module):\n    def __init__(self, embed_size, heads, dropout, forward_expansion=4):\n        super(TransformerLayer, self).__init__()\n        self.attention = MultiHeadAttention(embed_size, heads) \n        self.norm1 = nn.LayerNorm(embed_size)\n        self.norm2 = nn.LayerNorm(embed_size)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_size, forward_expansion * embed_size),\n            nn.ReLU(),\n            nn.Linear(forward_expansion * embed_size, embed_size),\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, value, key, query, mask):\n        # Values, Keys and Queries have size: (batch_size, query_len, embedding_size)\n        attention = self.attention(value, key, query, mask) # attention shape: (batch_size, query_len, embedding_size)\n        # Add skip connection, run through normalization and finally dropout\n        x = self.dropout(self.norm1(attention + query)) # x shape: (batch_size, query_len, embedding_size)\n        forward = self.feed_forward(x) # forward shape: (batch_size, query_len, embedding_size)\n        out = self.dropout(self.norm2(forward + x)) # out shape: (batch_size, query_len, embedding_size)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.306297Z","iopub.execute_input":"2023-07-19T20:08:40.306651Z","iopub.status.idle":"2023-07-19T20:08:40.325882Z","shell.execute_reply.started":"2023-07-19T20:08:40.306619Z","shell.execute_reply":"2023-07-19T20:08:40.324681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:#FF9B9B'>|</span> Encoder</b><a class='anchor' id='encoder'></a> [‚Üë](#top) \n***\n\n<br>\n<center><img style=\"float:left; margin:20px; padding:20px; max-height:250px\" src=\"https://i.imgur.com/rbEe0lW.png\"></center>\n<br>\n\nWe will be referring to the transformer block. The transformer block consists of:\n1. *Embedding*\n2. *Positional Encoding*\n3. *Transformer Block*","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, src_vocab_size, embed_size, num_layers, heads,\n        device, forward_expansion, dropout, max_length): \n        super(Encoder, self).__init__()\n        self.embed_size = embed_size # size of the input embedding\n        self.device = device # either \"cuda\" or \"cpu\"\n        # Lookup table with an embedding for each word in the vocabulary\n        self.word_embedding = nn.Embedding(src_vocab_size, embed_size) \n        # Lookup table with a positional embedding for each word in the sequence\n        self.position_embedding = nn.Embedding(max_length, embed_size)\n        self.layers = nn.ModuleList(\n            [\n                TransformerLayer(\n                    embed_size,\n                    heads,\n                    dropout=dropout,\n                    forward_expansion=forward_expansion,\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Forward pass.\n        :param x: source sequence. Shape: (batch_size, source_sequence_len).\n        :param mask: source mask is used when we need to pad the input.\n        :return output: torch tensor of shape (batch_size, src_sequence_length, embedding_size)\n        \"\"\"\n        batch_size, seq_length = x.shape\n        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n        out = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n        # In the Encoder the query, key, value are all the same, in the\n        # decoder this will change. This might look a bit odd in this case.\n        for layer in self.layers:\n            out = layer(out, out, out, mask)\n        # output shape: torch.Size([batch_size, sequence_length, embedding_size])\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.327662Z","iopub.execute_input":"2023-07-19T20:08:40.328007Z","iopub.status.idle":"2023-07-19T20:08:40.341559Z","shell.execute_reply.started":"2023-07-19T20:08:40.327966Z","shell.execute_reply":"2023-07-19T20:08:40.340565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#FF9B9B'>|</span> Decoder Layer</b><a class='anchor' id='decoder_block'></a> [‚Üë](#top) \n***\n\n<br>\n<center><img style=\"float:left; margin:20px; padding:20px; max-height:250px\" src=\"https://i.imgur.com/yV18zvn.png\"></center>\n<br>\n\nWe will be referring to the decoder layer. The decoder layer/block consists of:\n1. *Masked Multi-Head Attention*\n2. *Add & Norm*\n3. *Masked Multi-Head Attention*\n4. *Add & Norm*\n5. *Feed Forward*\n6. *Add & Norm*\n","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n        super(DecoderLayer, self).__init__()\n        self.norm = nn.LayerNorm(embed_size)\n        self.attention = MultiHeadAttention(embed_size, heads=heads)\n        self.transformer_block = TransformerLayer(\n            embed_size, heads, dropout, forward_expansion\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, value, key, src_mask, trg_mask):\n        \"\"\"\n        :param x: target input. Shape: (batch_size, target_sequence_len, embedding_size)\n        :param value: value extracted from encoder.\n        :param key: key extracted from encoder.\n        :param src_mask: source mask is used when we need to pad the input.\n        :param trg_mask: target mask is used to pass one element of the target at a time.\n        \"\"\"\n        \n        attention = self.attention(x, x, x, trg_mask)\n        query = self.dropout(self.norm(attention + x))\n        out = self.transformer_block(value, key, query, src_mask)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.342777Z","iopub.execute_input":"2023-07-19T20:08:40.343332Z","iopub.status.idle":"2023-07-19T20:08:40.358027Z","shell.execute_reply.started":"2023-07-19T20:08:40.343283Z","shell.execute_reply":"2023-07-19T20:08:40.357206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>7 <span style='color:#FF9B9B'>|</span> Decoder</b><a class='anchor' id='decoder'></a> [‚Üë](#top) \n***\n\n<br>\n<center><img style=\"float:left; margin:20px; padding:20px; max-height:350px\" src=\"https://i.imgur.com/jPMFhIK.png\"></center>\n<br>\n\nWe will be referring to the decoder. The decoder consists of:\n1. *Output Embedding*\n2. *Decoder Block*\n3. *Linear*\n4. *Softmax*\n\n**Notes:** \n\n- In this implementation the Token Embeddings are learned. Normally, we would use the output of the model's tokenizer.\n- In this implementation the Positional Embedding are learned. We don't use the formula.\n","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion,\n        dropout, device, max_length):\n        \"\"\"\n        :param trg_vocab_size: number of unique tokens in target vocabulary.\n        :param embed_size: size of output embeddings.\n        :param num_layers: number of DecoderLayers in the Decoder.\n        :param heads: number of heads in the MultiAttentionHeads inside the DecoderLayer.\n        :param forward_expansion: expansion factor in LinearLayer at the end of the TransformerLayer.\n        :param dropout: dropout probability.\n        :param device: either \"cuda\" or \"cpu\".\n        :param max_length: maximum length of sequence.\n        \"\"\"\n        super(Decoder, self).__init__()\n        self.device = device\n        #=== For each token in target vocab there is a token embedding ===\n        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size) \n        self.position_embedding = nn.Embedding(max_length, embed_size)\n        self.layers = nn.ModuleList(\n            [\n                DecoderLayer(embed_size, heads, forward_expansion, dropout, device)\n                for _ in range(num_layers)\n            ]\n        )\n        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, src_mask, trg_mask):\n        \"\"\"\n        :param x: target sequence. Shape: (batch_size, target_sequence_len)\n        :param enc_out: encoder output. Shape: (batch_size, src_sequence_length, embedding_size)\n        :param src_mask: source mask.\n        :param trg_mask: target mask.\n        \"\"\"\n        batch_size, seq_length = x.shape # x shape: (batch_size, target_sequence_len)\n        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device) # positions shape: (batch_size, target_sequence_len)\n        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n\n        for layer in self.layers:\n            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n\n        out = self.fc_out(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.35967Z","iopub.execute_input":"2023-07-19T20:08:40.360227Z","iopub.status.idle":"2023-07-19T20:08:40.375095Z","shell.execute_reply.started":"2023-07-19T20:08:40.360189Z","shell.execute_reply":"2023-07-19T20:08:40.374012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>8 <span style='color:#FF9B9B'>|</span> Transformer</b><a class='anchor' id='transformer'></a> [‚Üë](#top) \n***\n\n<br>\n<center><img src=\"https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg\" width=350></center>\n<br>\n","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512,\n                 num_layers=6, forward_expansion=4, heads=8, dropout=0, device=\"cpu\", max_length=100):\n\n        super(Transformer, self).__init__()\n        # === Encoder ===\n        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n        # === Decoder ===\n        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n        self.src_pad_idx = src_pad_idx\n        self.trg_pad_idx = trg_pad_idx\n        self.device = device\n\n    def make_src_mask(self, src):\n        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n        # (N, 1, 1, src_len)\n        return src_mask.to(self.device)\n\n    def make_trg_mask(self, trg):\n        \"\"\"\n        Returns a lower triangular matrix filled with 1s. The shape of the mask is (target_size, target_size).\n        Example: for a target of shape (batch_size=1, target_size=4)\n        tensor([[[[1., 0., 0., 0.],\n                  [1., 1., 0., 0.],\n                  [1., 1., 1., 0.],\n                  [1., 1., 1., 1.]]]])\n        \"\"\"\n        N, trg_len = trg.shape\n        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n            N, 1, trg_len, trg_len\n        )\n        return trg_mask.to(self.device)\n\n    def forward(self, src, trg):\n        src_mask = self.make_src_mask(src) # src_mask shape: \n        trg_mask = self.make_trg_mask(trg) # trg_mask shape: \n        enc_src = self.encoder(src, src_mask) # enc_src shape:\n        out = self.decoder(trg, enc_src, src_mask, trg_mask) # out shape: \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.376412Z","iopub.execute_input":"2023-07-19T20:08:40.377305Z","iopub.status.idle":"2023-07-19T20:08:40.392933Z","shell.execute_reply.started":"2023-07-19T20:08:40.377268Z","shell.execute_reply":"2023-07-19T20:08:40.391976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>9 <span style='color:#FF9B9B'>|</span> Example</b><a class='anchor' id='example'></a> [‚Üë](#top) \n***\n","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n    src_pad_idx = 0 # index of the padding token in source vocabulary\n    trg_pad_idx = 0 # index of the padding token in target vocabulary\n    src_vocab_size = 10 # number of unique tokens in source vocabulary\n    trg_vocab_size = 10 # number of unique tokens in target vocabulary\n    \n    print(f\"Input shape: {x.shape}\")\n    print(f\"Target shape: {trg.shape}\")\n    print(f\"Device available: {device}\")\n    \n    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n    out = model(x, trg[:, :-1])\n    print(f\"Output shape: {out.shape}\")\n    print(f\"Output: {out}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-19T20:08:40.394454Z","iopub.execute_input":"2023-07-19T20:08:40.395058Z","iopub.status.idle":"2023-07-19T20:08:41.209389Z","shell.execute_reply.started":"2023-07-19T20:08:40.395014Z","shell.execute_reply":"2023-07-19T20:08:41.208185Z"},"trusted":true},"execution_count":null,"outputs":[]}]}